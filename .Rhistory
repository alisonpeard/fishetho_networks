scale_y_reverse(breaks=c(1:10), labels=criteria.labels) +
scale_x_continuous(breaks=c(1:10), labels=criteria.labels) +
theme_minimal() +
theme(axis.text.x=element_text(angle=45, vjust=1, size=12, hjust=1)) +
theme(axis.text.y=element_text(angle=0, vjust=1, size=12, hjust=1)) +
coord_fixed()
mean.heatmap
# plotting standard deviations
melted.cormat <- melt(stds, na.rm = TRUE)
stds.heatmap <- ggplot(data=melted.cormat, aes(x=Var1, y=Var2, fill=value)) +
geom_tile(color="white") +
# geom_text(aes(label = round(value, 2))) +
scale_fill_gradient2(low="blue", high="red", mid="white",
midpoint=0.5, limit=c(0, 1), space="Lab",
name="std. dev") +
scale_y_reverse(breaks=c(1:10), labels=criteria.labels) +
scale_x_continuous(breaks=c(1:10), labels=criteria.labels) +
theme_minimal() +
theme(axis.text.x=element_text(angle=45, vjust=1, size=12, hjust=1)) +
theme(axis.text.y=element_text(angle=0, vjust=1, size=12, hjust=1)) +
coord_fixed()
stds.heatmap
srds
stds
library(wCorr)
library(ggplot2)
library(reshape2)
library(MASS)
# load the Python-processed files (still have negative values)
wd <- file.path("/Users", "alison", "Documents", "FishEtho", "data")
potential <- read.csv(file.path(wd, "potential.csv"))
likelihood <- read.csv(file.path(wd, "likelihood.csv"))
certainty <- read.csv(file.path(wd, "certainty.csv"))
# clean up negative values
likelihood[likelihood < 0] <- NA
potential[potential < 0] <- NA
certainty[certainty < 0] <- NA
criteria <- c("X17", "X18", "X29", "X30", "X31", "X32", "X33", "X34", "X35", "X36")
criteria.labels <- c("Home range", "Depth range", "Migration", "Reproduction",
"Aggregation", "Aggression", "Substrate", "Stress",
"Malformation", "Slaughter")
C <- length(criteria)
index <- rownames(likelihood)
# set up bootstrap
n <- 30  # size of bootstrap samples
betas = array(, c(C, C))
p.vals = array(, c(C, C))
for (i in 1:C){
for (j in 1:C){
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- likelihood[, column1]
y <- likelihood[, column2]
w <- certainty[, column1] + certainty[, column1] + 1
# get pairwise complete samples
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
if ((sum(x) > 0) && sum(y) >0){
lm.no.cert <- polr(y ~ x)
summary(lm.no.cert)
betas[i, j] <- summary(lm.no.cert)$coefficients[2,1]
p.vals[i, j] <- summary(lm.no.cert)$coefficients[2,4]
}
}
}
y
likeliihood
likelihood
likelihood / 2
y
y / 2
help("read.csv")
likelihood[criteria]
likelihood[criteria] <- likelihood[criteria] / 2
likelihood[criteria]
wd <- file.path("/Users", "alison", "Documents", "FishEtho", "data")
potential <- read.csv(file.path(wd, "potential.csv"))
likelihood <- read.csv(file.path(wd, "likelihood.csv"))
certainty <- read.csv(file.path(wd, "certainty.csv"))
# clean up negative values
likelihood[likelihood < 0] <- NA
potential[potential < 0] <- NA
certainty[certainty < 0] <- NA
criteria <- c("X17", "X18", "X29", "X30", "X31", "X32", "X33", "X34", "X35", "X36")
criteria.labels <- c("Home range", "Depth range", "Migration", "Reproduction",
"Aggregation", "Aggression", "Substrate", "Stress",
"Malformation", "Slaughter")
likelihood[criteria] <- as.factor(likelihood[criteria] / 2)
library(wCorr)
library(ggplot2)
library(reshape2)
library(MASS)
# load the Python-processed files (still have negative values)
wd <- file.path("/Users", "alison", "Documents", "FishEtho", "data")
potential <- read.csv(file.path(wd, "potential.csv"))
likelihood <- read.csv(file.path(wd, "likelihood.csv"))
certainty <- read.csv(file.path(wd, "certainty.csv"))
# clean up negative values
likelihood[likelihood < 0] <- NA
potential[potential < 0] <- NA
certainty[certainty < 0] <- NA
criteria <- c("X17", "X18", "X29", "X30", "X31", "X32", "X33", "X34", "X35", "X36")
criteria.labels <- c("Home range", "Depth range", "Migration", "Reproduction",
"Aggregation", "Aggression", "Substrate", "Stress",
"Malformation", "Slaughter")
likelihood[criteria] <- likelihood[criteria]
C <- length(criteria)
index <- rownames(likelihood)
# set up bootstrap
n <- 30  # size of bootstrap samples
betas = array(, c(C, C))
p.vals = array(, c(C, C))
for (i in 1:C){
for (j in 1:C){
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- as.factor(likelihood[, column1] / 2)
y <- as.factor(likelihood[, column2] / 2)
w <- certainty[, column1] + certainty[, column1] + 1
# get pairwise complete samples
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
if ((sum(x) > 0) && sum(y) >0){
lm.no.cert <- polr(y ~ x)
summary(lm.no.cert)
betas[i, j] <- summary(lm.no.cert)$coefficients[2,1]
p.vals[i, j] <- summary(lm.no.cert)$coefficients[2,4]
}
}
}
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- as.factor(likelihood[, column1] / 2)
y <- as.factor(likelihood[, column2] / 2)
w <- certainty[, column1] + certainty[, column1] + 1
# get pairwise complete samples
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
library(wCorr)
library(ggplot2)
library(reshape2)
library(MASS)
# load the Python-processed files (still have negative values)
wd <- file.path("/Users", "alison", "Documents", "FishEtho", "data")
potential <- read.csv(file.path(wd, "potential.csv"))
likelihood <- read.csv(file.path(wd, "likelihood.csv"))
certainty <- read.csv(file.path(wd, "certainty.csv"))
# clean up negative values
likelihood[likelihood < 0] <- NA
potential[potential < 0] <- NA
certainty[certainty < 0] <- NA
criteria <- c("X17", "X18", "X29", "X30", "X31", "X32", "X33", "X34", "X35", "X36")
criteria.labels <- c("Home range", "Depth range", "Migration", "Reproduction",
"Aggregation", "Aggression", "Substrate", "Stress",
"Malformation", "Slaughter")
likelihood[criteria] <- likelihood[criteria]
C <- length(criteria)
index <- rownames(likelihood)
# set up bootstrap
n <- 30  # size of bootstrap samples
betas = array(, c(C, C))
p.vals = array(, c(C, C))
for (i in 1:C){
for (j in 1:C){
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- likelihood[, column1] / 2
y <- likelihood[, column2] / 2
w <- certainty[, column1] + certainty[, column1] + 1
# get pairwise complete samples
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
if ((sum(x) > 0) && sum(y) > 0){
x <- as.factor(x)
y <- as.factor(y)
lm.no.cert <- polr(y ~ x)
summary(lm.no.cert)
betas[i, j] <- summary(lm.no.cert)$coefficients[2,1]
p.vals[i, j] <- summary(lm.no.cert)$coefficients[2,4]
}
}
}
?poly
for (i in 1:C){
for (j in 1:C){
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- likelihood[, column1] / 2
y <- likelihood[, column2] / 2
w <- certainty[, column1] + certainty[, column1] + 1
# get pairwise complete samples
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
if ((sum(x) > 0) && sum(y) > 0){
x <- as.factor(x)
y <- as.factor(y)
# lm.no.cert <- polr(y ~ x)  # for potential
lm.no.cert <- glm(y ~ x, family='binomial')
summary(lm.no.cert)
betas[i, j] <- summary(lm.no.cert)$coefficients[2,1]
p.vals[i, j] <- summary(lm.no.cert)$coefficients[2,4]
}
}
}
# plotting
melted.bmat <- melt(betas, na.rm = TRUE)
beta.heatmap <- ggplot(data=melted.bmat, aes(x=Var1, y=Var2, fill=value)) +
geom_tile(color="white") +
scale_fill_gradient2(low="blue", high="red", mid="white",
midpoint=0, limit=c(-1,1.01), space='Lab',
name="Weighted\nSpearman\nCorrelation") +
scale_y_reverse(breaks=c(1:10), labels=criteria.labels) +
scale_x_continuous(breaks=c(1:10), labels=criteria.labels) +
theme_minimal() +
theme(axis.text.x=element_text(angle=45, vjust=1, size=12, hjust=1)) +
theme(axis.text.y=element_text(angle=0, vjust=1, size=12, hjust=1)) +
coord_fixed()
beta.heatmap
bmat
betas
x
?polyr
??polyr
?polr
library(wCorr)
library(ggplot2)
library(reshape2)
library(MASS)
# load the Python-processed files (still have negative values)
wd <- file.path("/Users", "alison", "Documents", "FishEtho", "data")
potential <- read.csv(file.path(wd, "potential.csv"))
likelihood <- read.csv(file.path(wd, "likelihood.csv"))
certainty <- read.csv(file.path(wd, "certainty.csv"))
# clean up negative values
likelihood[likelihood < 0] <- NA
potential[potential < 0] <- NA
certainty[certainty < 0] <- NA
criteria <- c("X17", "X18", "X29", "X30", "X31", "X32", "X33", "X34", "X35", "X36")
criteria.labels <- c("Home range", "Depth range", "Migration", "Reproduction",
"Aggregation", "Aggression", "Substrate", "Stress",
"Malformation", "Slaughter")
likelihood[criteria] <- likelihood[criteria]
C <- length(criteria)
index <- rownames(likelihood)
# set up bootstrap
n <- 30  # size of bootstrap samples
betas = array(, c(C, C))
p.vals = array(, c(C, C))
for (i in 1:C){
for (j in 1:C){
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- likelihood[, column1] / 2
y <- likelihood[, column2] / 2
w <- certainty[, column1] + certainty[, column1] + 1
# get pairwise complete samples
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
if ((sum(x) > 0) && sum(y) > 0){
x <- as.factor(x)
y <- as.factor(y)
lm.no.cert <- polr(y ~ x)  # for potential
# lm.no.cert <- glm(y ~ x, family='binomial')
summary(lm.no.cert)
betas[i, j] <- summary(lm.no.cert)$coefficients[2,1]
p.vals[i, j] <- summary(lm.no.cert)$coefficients[2,4]
}
}
}
library(wCorr)
library(ggplot2)
library(reshape2)
library(MASS)
# load the Python-processed files (still have negative values)
wd <- file.path("/Users", "alison", "Documents", "FishEtho", "data")
potential <- read.csv(file.path(wd, "potential.csv"))
likelihood <- read.csv(file.path(wd, "likelihood.csv"))
certainty <- read.csv(file.path(wd, "certainty.csv"))
# clean up negative values
likelihood[likelihood < 0] <- NA
potential[potential < 0] <- NA
certainty[certainty < 0] <- NA
criteria <- c("X17", "X18", "X29", "X30", "X31", "X32", "X33", "X34", "X35", "X36")
criteria.labels <- c("Home range", "Depth range", "Migration", "Reproduction",
"Aggregation", "Aggression", "Substrate", "Stress",
"Malformation", "Slaughter")
likelihood[criteria] <- likelihood[criteria]
C <- length(criteria)
index <- rownames(likelihood)
# set up bootstrap
n <- 30  # size of bootstrap samples
betas = array(, c(C, C))
p.vals = array(, c(C, C))
for (i in 1:C){
for (j in 1:C){
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- likelihood[, column1] / 2
y <- likelihood[, column2] / 2
w <- certainty[, column1] + certainty[, column1] + 1
# get pairwise complete samples
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
if ((sum(x) > 0) && sum(y) > 0){
x <- x
y <- as.factor(y)
lm.no.cert <- polr(y ~ x)  # for potential
# lm.no.cert <- glm(y ~ x, family='binomial')
summary(lm.no.cert)
betas[i, j] <- summary(lm.no.cert)$coefficients[2,1]
p.vals[i, j] <- summary(lm.no.cert)$coefficients[2,4]
}
}
}
for (i in 1:C){
for (j in 1:C){
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- likelihood[, column1] / 2
y <- likelihood[, column2] / 2
w <- certainty[, column1] + certainty[, column1] + 1
# get pairwise complete samples
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
if ((sum(x) > 0) && sum(y) > 0){
x <- x
y <- as.factor(y)
# lm.no.cert <- polr(y ~ x)  # for potential
lm.no.cert <- glm(y ~ x, family='binomial')
summary(lm.no.cert)
betas[i, j] <- summary(lm.no.cert)$coefficients[2,1]
p.vals[i, j] <- summary(lm.no.cert)$coefficients[2,4]
}
}
}
betas
exp(betas)
p.vsummary(lm.no.cert)
summary(lm.no.cert)
library(wCorr)
library(ggplot2)
library(reshape2)
# load the Python-processed files (still have negative values)
wd <- file.path("/Users", "alison", "Documents", "FishEtho", "data")
potential <- read.csv(file.path(wd, "potential.csv"))
likelihood <- read.csv(file.path(wd, "likelihood.csv"))
certainty <- read.csv(file.path(wd, "certainty.csv"))
# clean up negative values
likelihood[likelihood < 0] <- NA
potential[potential < 0] <- NA
certainty[certainty < 0] <- NA
criteria <- c("X17", "X18", "X29", "X30", "X31", "X32", "X33", "X34", "X35", "X36")
criteria.labels <- c("Home range", "Depth range", "Migration", "Reproduction",
"Aggregation", "Aggression", "Substrate", "Stress",
"Malformation", "Slaughter")
C <- length(criteria)
N <- nrow(likelihood)
index <- rownames(likelihood)
wcorrs <- array(, c(C, C))
present.counts <- array(, c(C, C))
for (i in 1:C){
for (j in 1:i + 1){
# what to calculate correlation for
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- likelihood[, column1]
y <- likelihood[, column2]
w <- certainty[, column1] + certainty[, column1] + 1
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
# set up nan-correcting
p = length(ids)
present.counts[i, j] <- p / N
present.counts[j, i] <- p / N
if (p > 0){
# calculate weighted correlation (NaN for all zeros)
wcorr <- weightedCorr(x, y, method="Spearman", weights=w)
wcorrs[i, j] <- wcorr
wcorrs[j, i] <- wcorr
}
}
}
library(wCorr)
library(ggplot2)
library(reshape2)
# load the Python-processed files (still have negative values)
wd <- file.path("/Users", "alison", "Documents", "FishEtho", "data")
potential <- read.csv(file.path(wd, "potential.csv"))
likelihood <- read.csv(file.path(wd, "likelihood.csv"))
certainty <- read.csv(file.path(wd, "certainty.csv"))
# clean up negative values
likelihood[likelihood < 0] <- NA
potential[potential < 0] <- NA
certainty[certainty < 0] <- NA
criteria <- c("X17", "X18", "X29", "X30", "X31", "X32", "X33", "X34", "X35", "X36")
criteria.labels <- c("Home range", "Depth range", "Migration", "Reproduction",
"Aggregation", "Aggression", "Substrate", "Stress",
"Malformation", "Slaughter")
C <- length(criteria)
N <- nrow(likelihood)
index <- rownames(likelihood)
wcorrs <- array(, c(C, C))
present.counts <- array(, c(C, C))
for (i in 1:C){
for (j in 1:i + 1){
# what to calculate correlation for
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- likelihood[, column1]
y <- likelihood[, column2]
w <- certainty[, column1] + certainty[, column1] + 1
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
# set up nan-correcting
p = length(ids)
present.counts[i, j] <- p / N
present.counts[j, i] <- p / N
if (p > 0){
# calculate weighted correlation (NaN for all zeros)
wcorr <- weightedCorr(x, y, method="Spearman", weights=w)
wcorrs[i, j] <- wcorr
wcorrs[j, i] <- wcorr
}
}
}
colmn1
column1
column2
for (i in 1:C){
for (j in 1:C){
# what to calculate correlation for
column1 <- criteria[i]
column2 <- criteria[j]
# get vars (removing NULLS)
x <- likelihood[, column1]
y <- likelihood[, column2]
w <- certainty[, column1] + certainty[, column1] + 1
idx <- which(!is.na(x))
idy <- which(!is.na(y))
ids <- intersect(idx, idy)
x <- x[ids]
y <- y[ids]
w <- w[ids]
# set up nan-correcting
p = length(ids)
present.counts[i, j] <- p / N
present.counts[j, i] <- p / N
if (p > 0){
# calculate weighted correlation (NaN for all zeros)
wcorr <- weightedCorr(x, y, method="Spearman", weights=w)
wcorrs[i, j] <- wcorr
wcorrs[j, i] <- wcorr
}
}
}
wcorrs
write.csv(wcorrs, "/Users/alison/Documents/FishEtho/data/spearman/total_corrs.csv")
write.csv(present.counts, "/Users/alison/Documents/FishEtho/data/spearman/percentage_present.csv")
write.csv(wcorrs, "/Users/alison/Documents/FishEtho/data/spearman/total_corrs.csv")
write.csv(present.counts, "/Users/alison/Documents/FishEtho/data/spearman/percentage_present.csv")
write.csv(wcorrs, "/Users/alison/Documents/FishEtho/data/spearman/likelihood/total_corrs.csv")
write.csv(present.counts, "/Users/alison/Documents/FishEtho/data/spearman/likelihood/percentage_present.csv")
